{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stmulugheta/20223-ML-DL-models/blob/master/PyTorch_Lab_GNNs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DM Lab on GNN with PyTorch and PyTorch Geometric\n",
        "\n",
        "Lab notebook by Andrea Mastropietro, inspire by content [here](https://towardsdatascience.com/hands-on-graph-neural-networks-with-pytorch-pytorch-geometric-359487e221a8).\n",
        "\n",
        "In this lab we will see how to handle graph data and feed them to GNNs. We will use PyG, a library that stands upon PyTorch for working with graph data and graph neural networks. It allows yoiu to oute out-of-the-shelf graph layers or to define your own by using the basic components of graph neural networks, message passing, propagation."
      ],
      "metadata": {
        "id": "yahEoEPAVPz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install neeeded libraries woth the correct versions of CUDA and PyTorch"
      ],
      "metadata": {
        "id": "Wit6H4wxekyG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-1.10.0+cu113.html\n",
        "!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-1.10.0+cu113.html\n",
        "!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git"
      ],
      "metadata": {
        "id": "kBByjLyeekGh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Example: What does PyG want?"
      ],
      "metadata": {
        "id": "OxD5BLedbNCX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![picture](https://drive.google.com/uc?id=1BD2-3qqzb_WsY2swP8VGNqJVSsWgqYjV)\n",
        "\n",
        "(image from [here](https://towardsdatascience.com/hands-on-graph-neural-networks-with-pytorch-pytorch-geometric-359487e221a8))"
      ],
      "metadata": {
        "id": "rXnQfgEpaAQm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We want to create the graph we see above using PyTorch tensors"
      ],
      "metadata": {
        "id": "kRfqJB76aNIJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "0BZjOP5wXlOM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have four nodes with feature vectors and labels"
      ],
      "metadata": {
        "id": "xhNOOVadbCRg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nNvQyZZSVFhN"
      },
      "outputs": [],
      "source": [
        "x = torch.tensor([[2,1], [5,6], [3,7], [12,0]], dtype=torch.float)\n",
        "y = torch.tensor([0, 1, 0, 1], dtype=torch.float)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need now to define the edges.\n",
        "\n",
        "PyG wants edges in COO format (coordinate format): two lists, the first one with soruce nodes and the second one with target nodes. The order of the edge index does not matter, we need only to know which are the edges, there is no ordering among them."
      ],
      "metadata": {
        "id": "IeV75SQYbWBG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "edge_index = torch.tensor([[0, 1, 2, 0, 3],\n",
        "                           [1, 0, 1, 3, 2]], dtype=torch.long)"
      ],
      "metadata": {
        "id": "1w9CTpL2bVWA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's define a Data object for our PyTorch model"
      ],
      "metadata": {
        "id": "MBHEVs2meTiW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.data import Data\n",
        "\n",
        "data = Data(x=x, y=y, edge_index=edge_index)"
      ],
      "metadata": {
        "id": "feVoBtuMeED0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can convert the ataset to networkX graph in order to visualzie it"
      ],
      "metadata": {
        "id": "tO8g-YaUikc1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.utils import to_networkx\n",
        "import networkx as nx\n",
        "\n",
        "G = to_networkx(data, to_undirected=False)\n",
        "nx.draw(G, with_labels=True)"
      ],
      "metadata": {
        "id": "N00pdSjNiiwf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Karate Club - Node Classification with Predefined GCNConv layers\n",
        "\n",
        "We will know see how to perfomr basic node classification by creating a neural netowk using the layers altredy implemented by PyG. We will biuld a 2-layered GCN. The reference paper for this model is [here](https://arxiv.org/abs/1609.02907).\n",
        "\n",
        "We will use a simple netwrok, Zachary's Karate. The nodes represent a member of the club and two memeber are connected if they interacted. The club was split due to internat conlfilts, and the labels indicate if e member followed the instruction, \"Mr Hi\", or not. This is a social interaction dataset. This is a very small dataset, we do not expect amazing results :)"
      ],
      "metadata": {
        "id": "9Ct_qyZcVvjr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# load graph from networkx library\n",
        "G = nx.karate_club_graph()\n",
        "print(nx.info(G))\n",
        "\n",
        "nx.draw(G, with_labels=True)"
      ],
      "metadata": {
        "id": "_g7ZQSzIVy8q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# retrieve the labels for each node\n",
        "labels = np.asarray([G.nodes[i]['club'] != 'Mr. Hi' for i in G.nodes]).astype(np.int64)\n",
        "\n",
        "# create edge index. We need to have data as previously shown. We can exploit networkX and scipy for that \n",
        "adj = nx.to_scipy_sparse_matrix(G).tocoo() #coordinate format\n",
        "print(adj)"
      ],
      "metadata": {
        "id": "iuMmAXjQ-HmW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create edge index in the proper way\n",
        "row = torch.from_numpy(adj.row.astype(np.int64)).to(torch.long)\n",
        "col = torch.from_numpy(adj.col.astype(np.int64)).to(torch.long)\n",
        "edge_index = torch.stack([row, col], dim=0)\n",
        "\n",
        "display(edge_index)"
      ],
      "metadata": {
        "id": "MzTpNTzcALM9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# using degree as embedding. For simplicity, the feature vector describing the \n",
        "# will be just its degree, which is enough for us\n",
        "embeddings = np.array(list(dict(G.degree()).values()))\n",
        "\n",
        "# normalizing degree values\n",
        "scale = StandardScaler()\n",
        "embeddings = scale.fit_transform(embeddings.reshape(-1,1))\n",
        "print(embeddings)"
      ],
      "metadata": {
        "id": "GctyVXGIAbXV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create Custom Dataset\n",
        "\n",
        "In order to create datastets with PyG you can use InMemoryDataset and Dataset. Similar to TorchVision, the first one is used for data that fit in main memory, the second one can hendel larger datasets by reading batches from your disk. We will use the fisrt one now, but the implementation is similar.\n",
        "\n",
        "We will extend the class InMemoryDataset to create our own dataset class."
      ],
      "metadata": {
        "id": "X96FBhu6V94R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from torch_geometric.data import InMemoryDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch_geometric.transforms as T\n",
        "\n",
        "# custom dataset\n",
        "class KarateDataset(InMemoryDataset):\n",
        "    def __init__(self, transform=None):\n",
        "        super(KarateDataset, self).__init__('.', transform, None, None) #pre transform and pre filter: None, we don't need them\n",
        "\n",
        "        data = Data(edge_index=edge_index)\n",
        "        \n",
        "        data.num_nodes = G.number_of_nodes()\n",
        "        \n",
        "        # embedding \n",
        "        data.x = torch.from_numpy(embeddings).type(torch.float32)\n",
        "        \n",
        "        # labels\n",
        "        y = torch.from_numpy(labels).type(torch.long)\n",
        "        data.y = y.clone().detach() #removing tensors computational graph for efficency since it is not needed\n",
        "        \n",
        "        data.num_classes = 2\n",
        "\n",
        "        # splitting the data into train, validation and test\n",
        "        X_train, X_test, y_train, y_test = train_test_split(pd.Series(G.nodes()), \n",
        "                                                            pd.Series(labels),\n",
        "                                                            test_size=0.30, \n",
        "                                                            random_state=42)\n",
        "        \n",
        "        n_nodes = G.number_of_nodes()\n",
        "        \n",
        "        # create train and test masks for data\n",
        "        train_mask = torch.zeros(n_nodes, dtype=torch.bool)\n",
        "        test_mask = torch.zeros(n_nodes, dtype=torch.bool)\n",
        "        train_mask[X_train.index] = True\n",
        "        test_mask[X_test.index] = True\n",
        "        data['train_mask'] = train_mask\n",
        "        data['test_mask'] = test_mask\n",
        "\n",
        "        self.data, self.slices = self.collate([data])\n",
        "\n",
        "    # def _download(self):\n",
        "    #     return\n",
        "\n",
        "    # def _process(self):\n",
        "    #     return\n",
        "\n",
        "    # def __repr__(self):\n",
        "    #     return '{}()'.format(self.__class__.__name__)\n",
        "    \n",
        "dataset = KarateDataset()\n",
        "data = dataset[0]"
      ],
      "metadata": {
        "id": "ci-vf1HNWBPe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "id": "Wvh58wHqeJEO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv\n",
        "\n",
        "# GCN model with 2 layers \n",
        "class Net(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = GCNConv(data.num_features, 16) #in feature, out dim\n",
        "        self.conv2 = GCNConv(16, int(data.num_classes))\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "        x = F.relu(self.conv1(x, edge_index))\n",
        "        x = F.dropout(x, training=self.training)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "data =  data.to(device)\n",
        "\n",
        "model = Net().to(device) "
      ],
      "metadata": {
        "id": "wZWXoBuha1Sm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "\n",
        "torch.manual_seed(42)\n",
        "\n",
        "#optimizer_name = \"Adam\"\n",
        "lr = 1e-1\n",
        "#optimizer = getattr(torch.optim, optimizer_name)(model.parameters(), lr=lr)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "epochs = 200\n",
        "\n",
        "def train():\n",
        "  model.train()\n",
        "  optimizer.zero_grad()\n",
        "  #negative log likelihood, we need it after a softmax output activation function\n",
        "  #we also tell to compute the loss using the  mask (so, on the train samples) \n",
        "  F.nll_loss(model(data)[data.train_mask], data.y[data.train_mask]).backward() \n",
        "  optimizer.step()\n",
        "  \n",
        "  # comment the following for avoiding eval during training\n",
        "  model.eval()\n",
        "  logits = model(data)\n",
        "  train_mask = data['train_mask']\n",
        "  train_pred = logits[train_mask].max(1)[1]\n",
        "  train_acc = train_pred.eq(data.y[train_mask]).sum().item() / train_mask.sum().item() #eq: elementwise equality\n",
        "\n",
        "  return train_acc\n",
        "  \n",
        "\n",
        "@torch.no_grad()\n",
        "def test():\n",
        "  model.eval()\n",
        "  logits = model(data)\n",
        "\n",
        "  # uncomment the following if you want to eval on the train and test together\n",
        "  # train_mask = data['train_mask']\n",
        "  # train_pred = logits[train_mask].max(1)[1]\n",
        "  # train_acc = train_pred.eq(data.y[train_mask]).sum().item() / train_mask.sum().item()\n",
        "\n",
        "  test_mask = data['test_mask']\n",
        "  test_pred = logits[test_mask].max(1)[1]\n",
        "  test_acc = test_pred.eq(data.y[test_mask]).sum().item() / test_mask.sum().item()\n",
        "\n",
        "  return test_acc\n",
        "\n",
        "from tqdm import tqdm, tqdm_notebook\n",
        "\n",
        "for epoch in tqdm_notebook(range(1, epochs)):\n",
        "  train_acc = train()\n",
        "\n",
        "  \n",
        "\n",
        "test_acc = test()\n",
        "\n",
        "print('#' * 70)\n",
        "print('Train Accuracy: %s' % train_acc)\n",
        "print('Test Accuracy: %s' % test_acc)\n",
        "print('#' * 70)"
      ],
      "metadata": {
        "id": "cDwpIYyBa5cQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Explainability - GNNExplainer\n",
        "\n",
        "why was a node classified as this or that? Explainability methods try to tell us that. GNNExplainer is the first exmplainability method for GNNs. It retrieves an explanatipn graph by finding the most importnat edges for the given prediction.\n",
        "\n",
        "We will usethe PyG implemantation rather that the official one. The corresponding paper is [here](https://arxiv.org/abs/1903.03894)."
      ],
      "metadata": {
        "id": "_y7YGqEnJwP7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os.path as osp\n",
        "\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from torch_geometric.nn import GCNConv, GNNExplainer\n",
        "\n",
        "torch.manual_seed(42)\n",
        "\n",
        "class Net(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = GCNConv(dataset.num_features, 16, normalize=False)\n",
        "        self.conv2 = GCNConv(16, dataset.num_classes, normalize=False)\n",
        "\n",
        "    def forward(self, x, edge_index, edge_weight):\n",
        "        x = F.relu(self.conv1(x, edge_index, edge_weight))\n",
        "        x = F.dropout(x, training=self.training)\n",
        "        x = self.conv2(x, edge_index, edge_weight)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = Net().to(device)\n",
        "data = data.to(device)\n",
        "lr = 1e-1\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "epochs = 200\n",
        "\n",
        "x, edge_index, edge_weight = data.x, data.edge_index, None\n",
        "\n",
        "num_epochs = 200\n",
        "\n",
        "for epoch in tqdm(range(num_epochs)):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    log_logits = model(x, edge_index, edge_weight)\n",
        "    loss = F.nll_loss(log_logits[data.train_mask], data.y[data.train_mask])\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "with torch.no_grad():\n",
        "  model.eval()\n",
        "  logits = model(x, edge_index, edge_weight)\n",
        "\n",
        "  #test_mask = data['test_mask']\n",
        "  preds = logits.max(1)[1]\n"
      ],
      "metadata": {
        "id": "MPAxw5zaJvTh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "explainer = GNNExplainer(model, epochs=200, return_type='log_prob', num_hops = 2) #if num_hops is none it is detected from the num of message passing ops.\n",
        "                                                                                  #it is needed to tell the explainer \"how far to go\" to look for explanations.\n",
        "node_idx = 10 #node to explain 0, 10... node 0 is Mr Hi\n",
        "pred_node_idx = preds[node_idx].item()\n",
        "print(\"Explaining node \", node_idx, \" with predicted class: \", pred_node_idx)\n",
        "\n",
        "node_feat_mask, edge_mask = explainer.explain_node(node_idx, x, edge_index,\n",
        "                                                   edge_weight=edge_weight)"
      ],
      "metadata": {
        "id": "aCDf2JTaUwDI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ax, G = explainer.visualize_subgraph(node_idx, edge_index, edge_mask, y=data.y, seed = node_idx, threshold = None) #you can set threshold to define the hard mask accordign to the sparisty you wnat ot obtain and how much you want to be strict\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7KECTmIvUgEL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "NOTE: you have directions now even if the original graph was undirected because, when creating the edge mask, you can consider the infomration flow from both directions of the message passing operation. We have to say that subgraphs retrieved by GNNExplainer are not guaranteed to be connected! If you consider this propery desirable for your application, then your choice could be SubgraphX ;) Since a GNN aggregate info up to N hops, even edges not connected to the explainee node are meaningful, since the info actually arrives to our node! So we see which are the most imporatn connections for the prediction, with aggregate info arrives to our node.\n",
        "\n",
        "Usually, when exlaining node 0 (Mr. Hi) no real explanation info in conveyed, since he is the founder of the new club and he moslty influenced others."
      ],
      "metadata": {
        "id": "eLcPvYs8Pe6j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Custon Model Definition - Node Classification with GraphSAGE on RecSYS datasets\n",
        "\n",
        "We will see how to build a GraphSAGE layer, from the correspoding [paper](https://arxiv.org/abs/1706.02216). This model is already implemented in PyG, but we will build it from stratch to see that we can define our own GNN conv layers :)\n",
        "\n",
        "We will use data from the 2015 Recommander Systems Challenge "
      ],
      "metadata": {
        "id": "sutwC9CmrLBb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Loading\n",
        "\n",
        "We will load the data and define a custom Data class, as before.\n",
        "\n",
        "Those data are divived into two csvs yoochoose-clicks.dat, and yoochoose-buys.dat, containing click events and buy events, respectively and we need  to see if a click event was followedby a buy event. "
      ],
      "metadata": {
        "id": "J3e5ssNdeJdg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "ZkHzTywxi9BK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "#your GDrive path here :)\n",
        "CLICKS_PATH = \"/content/gdrive/MyDrive/University/PhD/Teaching/2021-2022/Data Mining/Labs/Lab 4 - GNNs with PyG/data/yoochoose-clicks.dat\"\n",
        "BUYS_PATH = \"/content/gdrive/MyDrive/University/PhD/Teaching/2021-2022/Data Mining/Labs/Lab 4 - GNNs with PyG/data/yoochoose-buys.dat\"\n",
        "\n",
        "df_clicks = pd.read_csv(CLICKS_PATH, header=None) \n",
        "display(df_clicks.head())\n",
        "df_clicks.columns=['session_id','timestamp','item_id','category']\n",
        "\n",
        "df_buys = pd.read_csv(BUYS_PATH, header=None) \n",
        "df_buys.columns=['session_id','timestamp','item_id','price','quantity']\n",
        "\n",
        "item_encoder = LabelEncoder()\n",
        "df_clicks['item_id'] = item_encoder.fit_transform(df_clicks.item_id)\n",
        "display(df_clicks.head())"
      ],
      "metadata": {
        "id": "dn46x0_veIqm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#we taka a random sample of the dataset since it is to large\n",
        "import numpy as np\n",
        "\n",
        "sampled_session_id = np.random.choice(df_clicks.session_id.unique(), 10000, replace=False)\n",
        "df_clicks = df_clicks.loc[df_clicks.session_id.isin(sampled_session_id)]\n",
        "print(df_clicks.nunique())"
      ],
      "metadata": {
        "id": "GczE2ocokeQS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To determine the ground truth we simply check if a session_id in yoochoose-clicks presents in yoochoose-buys (so if a click event led to a buy event)"
      ],
      "metadata": {
        "id": "QmL6LhtJnp7o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_clicks['label'] = df_clicks.session_id.isin(df_buys.session_id)\n",
        "df_clicks.head()"
      ],
      "metadata": {
        "id": "y1-bZhU-oBPh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Building the Dataset\n",
        "\n",
        "We tranform our data to a  Dataset object. We treat each item in a session as a node: all items in the same session form a graph. Thus, we need to group the data by session_id and iterate over these groups. In each iteration, the item_id in each group are categorically encoded again since for each graph, the node index should count from 0."
      ],
      "metadata": {
        "id": "XzBKcUkioIvT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch_geometric.data import InMemoryDataset\n",
        "from tqdm import tqdm\n",
        "\n",
        "class YooChooseDataset(InMemoryDataset):\n",
        "    def __init__(self, root, transform=None, pre_transform=None):\n",
        "        super(YooChooseDataset, self).__init__(root, transform, pre_transform)\n",
        "\n",
        "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
        "\n",
        "    #It returns a list that shows a list of raw, unprocessed file names\n",
        "    @property\n",
        "    def raw_file_names(self):\n",
        "        return []\n",
        "\n",
        "    #returns a list containing the file names of all the processed data    \n",
        "    @property\n",
        "    def processed_file_names(self):\n",
        "        return ['processed_data.dat']\n",
        "\n",
        "    #This function should download the data you are working on to the directory as specified in self.raw_dir we don't need it\n",
        "    def download(self):\n",
        "        pass\n",
        "\n",
        "    #This is the most important method of Dataset. You need to gather your data into a list of Data objects. Then, call self.collate() to compute the slices that will be used by the DataLoader object.\n",
        "    # We need this since we are working with data consituted by multiple sessions/graphs, so we need to collate them into a single Data objects  \n",
        "    def process(self):\n",
        "        \n",
        "        data_list = []\n",
        "\n",
        "        # process by session_id\n",
        "        grouped = df_clicks.groupby('session_id')\n",
        "        for session_id, group in tqdm(grouped):\n",
        "            sess_item_id = LabelEncoder().fit_transform(group.item_id)\n",
        "            group = group.reset_index(drop=True)\n",
        "            group['sess_item_id'] = sess_item_id\n",
        "            node_features = group.loc[group.session_id==session_id,['sess_item_id','item_id']].sort_values('sess_item_id').item_id.drop_duplicates().values\n",
        "\n",
        "            node_features = torch.LongTensor(node_features).unsqueeze(1)\n",
        "            target_nodes = group.sess_item_id.values[1:]\n",
        "            source_nodes = group.sess_item_id.values[:-1]\n",
        "\n",
        "            edge_index = torch.tensor([source_nodes,\n",
        "                                   target_nodes], dtype=torch.long)\n",
        "            x = node_features\n",
        "\n",
        "            y = torch.FloatTensor([group.label.values[0].astype(int)])\n",
        "\n",
        "            data = Data(x=x, edge_index=edge_index, y=y)\n",
        "            data_list.append(data)\n",
        "        \n",
        "        data, slices = self.collate(data_list)\n",
        "        torch.save((data, slices), self.processed_paths[0])"
      ],
      "metadata": {
        "id": "v8k85zSyohM9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We shuffle the data and split into train, test and validation sets. We do it outside the data class now to see a different approach, but you can do it the same way as before by defining masks."
      ],
      "metadata": {
        "id": "r2ljmRWVpcxC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = YooChooseDataset(root = \"../\")"
      ],
      "metadata": {
        "id": "P4h0Q-GxpvUC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "id": "qErzfSql3ZMc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#data = dataset[0]\n",
        "dataset = dataset.shuffle()\n",
        "train_data = dataset[:8000]\n",
        "val_data = dataset[8000:9000]\n",
        "test_data = dataset[9000:]\n",
        "len(train_data), len(val_data), len(test_data)"
      ],
      "metadata": {
        "id": "JAo6P17Ou5L2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We define a DataLoader"
      ],
      "metadata": {
        "id": "MKJuwDl83jOH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.loader import DataLoader\n",
        "batch_size= 512\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size)\n",
        "val_loader = DataLoader(val_data, batch_size=batch_size)\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "8B9Xsakt3gnP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_items = df_clicks.item_id.max() + 1 \n",
        "num_items"
      ],
      "metadata": {
        "id": "rXu2_Fs_3w7o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Custom Layer Definition - GraphSAGE\n",
        "\n",
        "Here comes the hard part :) Let's build our custom  layer as a GraphSAGE layer."
      ],
      "metadata": {
        "id": "u-DfSMK64iU3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn import Sequential as Seq, Linear, ReLU\n",
        "from torch_geometric.nn import MessagePassing\n",
        "from torch_geometric.utils import remove_self_loops, add_self_loops\n",
        "\n",
        "class SAGEConv(MessagePassing):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(SAGEConv, self).__init__(aggr='max') #  \"max\" aggregation. This is the aggregation type used in GraphSAGE\n",
        "        self.lin = torch.nn.Linear(in_channels, out_channels)\n",
        "        self.act = torch.nn.ReLU()\n",
        "        self.update_lin = torch.nn.Linear(in_channels + out_channels, in_channels, bias=False)\n",
        "        self.update_act = torch.nn.ReLU()\n",
        "        \n",
        "    def forward(self, x, edge_index):\n",
        "        # x has shape [N, in_channels]\n",
        "        # edge_index has shape [2, E]\n",
        "        \n",
        "        \n",
        "        edge_index, _ = remove_self_loops(edge_index)\n",
        "        edge_index, _ = add_self_loops(edge_index, num_nodes=x.size(0))\n",
        "        \n",
        "        \n",
        "        return self.propagate(edge_index, size=(x.size(0), x.size(0)), x=x)\n",
        "\n",
        "    def message(self, x_j):\n",
        "        # x_j has shape [E, in_channels]\n",
        "\n",
        "        x_j = self.lin(x_j)\n",
        "        x_j = self.act(x_j)\n",
        "        \n",
        "        return x_j\n",
        "\n",
        "    def update(self, aggr_out, x):\n",
        "        # aggr_out has shape [N, out_channels]\n",
        "\n",
        "\n",
        "        new_embedding = torch.cat([aggr_out, x], dim=1)\n",
        "        \n",
        "        new_embedding = self.update_lin(new_embedding)\n",
        "        new_embedding = self.update_act(new_embedding)\n",
        "        \n",
        "        return new_embedding"
      ],
      "metadata": {
        "id": "BilEtbPHrTmo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Network Definition using GraphSAGE layer\n",
        "\n",
        "We now build the whole GrapHSAGE network from the reference paper using the layer just defined."
      ],
      "metadata": {
        "id": "6Wpr8uw7b1gb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.nn import TopKPooling\n",
        "from torch_geometric.nn import global_mean_pool as gap, global_max_pool as gmp\n",
        "import torch.nn.functional as F\n",
        "\n",
        "embed_dim = 128\n",
        "\n",
        "class Net(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "\n",
        "        self.conv1 = SAGEConv(embed_dim, 128)\n",
        "        self.pool1 = TopKPooling(128, ratio=0.8)\n",
        "        self.conv2 = SAGEConv(128, 128)\n",
        "        self.pool2 = TopKPooling(128, ratio=0.8)\n",
        "        self.conv3 = SAGEConv(128, 128)\n",
        "        self.pool3 = TopKPooling(128, ratio=0.8)\n",
        "        self.item_embedding = torch.nn.Embedding(num_embeddings=num_items, embedding_dim=embed_dim)\n",
        "        self.lin1 = torch.nn.Linear(256, 128)\n",
        "        self.lin2 = torch.nn.Linear(128, 64)\n",
        "        self.lin3 = torch.nn.Linear(64, 1)\n",
        "        self.bn1 = torch.nn.BatchNorm1d(128)\n",
        "        self.bn2 = torch.nn.BatchNorm1d(64)\n",
        "        self.act1 = torch.nn.ReLU()\n",
        "        self.act2 = torch.nn.ReLU()        \n",
        "  \n",
        "    def forward(self, data):\n",
        "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
        "        x = self.item_embedding(x)\n",
        "        x = x.squeeze(1)        \n",
        "\n",
        "        x = F.relu(self.conv1(x, edge_index))\n",
        "\n",
        "        x, edge_index, _, batch, _, _ = self.pool1(x, edge_index, None, batch)\n",
        "        x1 = torch.cat([gmp(x, batch), gap(x, batch)], dim=1)\n",
        "\n",
        "        x = F.relu(self.conv2(x, edge_index))\n",
        "     \n",
        "        x, edge_index, _, batch, _, _ = self.pool2(x, edge_index, None, batch)\n",
        "        x2 = torch.cat([gmp(x, batch), gap(x, batch)], dim=1)\n",
        "\n",
        "        x = F.relu(self.conv3(x, edge_index))\n",
        "\n",
        "        x, edge_index, _, batch, _ , _= self.pool3(x, edge_index, None, batch)\n",
        "        x3 = torch.cat([gmp(x, batch), gap(x, batch)], dim=1)\n",
        "\n",
        "        x = x1 + x2 + x3\n",
        "\n",
        "        x = self.lin1(x)\n",
        "        x = self.act1(x)\n",
        "        x = self.lin2(x)\n",
        "        x = self.act2(x)      \n",
        "        x = F.dropout(x, p=0.5, training=self.training)\n",
        "\n",
        "        x = torch.sigmoid(self.lin3(x)).squeeze(1)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "OgWbBhljb54c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define optimizer and loss criterion"
      ],
      "metadata": {
        "id": "aB8-uJsN6E77"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda')\n",
        "model = Net().to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
        "crit = torch.nn.BCELoss()"
      ],
      "metadata": {
        "id": "EdquJjQO51tm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define train and test functions\n",
        "\n",
        "We use the roc auc as metric, since the dataset is higly umbalanced and using accuracy could be not really meaninful."
      ],
      "metadata": {
        "id": "oIjLywVt6LiH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "def train():\n",
        "    model.train()\n",
        "\n",
        "    loss_all = 0\n",
        "    for data in train_loader:\n",
        "        data = data.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        label = data.y.to(device)\n",
        "        loss = crit(output, label)\n",
        "        loss.backward()\n",
        "        loss_all += data.num_graphs * loss.item()\n",
        "        optimizer.step()\n",
        "    return loss_all / len(train_data)\n",
        "\n",
        "\n",
        "def evaluate(loader):\n",
        "    model.eval()\n",
        "\n",
        "    predictions = []\n",
        "    labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data in loader:\n",
        "\n",
        "            data = data.to(device)\n",
        "            pred = model(data).detach().cpu().numpy()\n",
        "\n",
        "            label = data.y.detach().cpu().numpy()\n",
        "            predictions.append(pred)\n",
        "            labels.append(label)\n",
        "\n",
        "    predictions = np.hstack(predictions)\n",
        "    labels = np.hstack(labels)\n",
        "    \n",
        "    return roc_auc_score(labels, predictions)"
      ],
      "metadata": {
        "id": "VeCx22qG6Bin"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in tqdm(range(10)):\n",
        "    loss = train()\n",
        "    train_acc = evaluate(train_loader)\n",
        "    val_acc = evaluate(val_loader)    \n",
        "    test_acc = evaluate(test_loader)\n",
        "    print('Epoch: {:03d}, Loss: {:.5f}, Train Auc: {:.5f}, Val Auc: {:.5f}, Test Auc: {:.5f}'.\n",
        "          format(epoch, loss, train_acc, val_acc, test_acc))"
      ],
      "metadata": {
        "id": "GADCiS136f_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extra - GNNExplainer on Cora Dataset\n"
      ],
      "metadata": {
        "id": "dmKRXeX-yer6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os.path as osp\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "from torch_geometric.datasets import Planetoid\n",
        "import torch_geometric.transforms as T\n",
        "from torch_geometric.nn import GCNConv, GNNExplainer\n",
        "\n",
        "dataset = 'Cora'\n",
        "path = osp.join(osp.dirname(osp.realpath(\"content/\")), '..', 'data', 'Planetoid')\n",
        "transform = T.Compose([T.GCNNorm(), T.NormalizeFeatures()])\n",
        "dataset = Planetoid(path, dataset, transform=transform)\n",
        "data = dataset[0]\n",
        "\n",
        "\n",
        "class Net(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = GCNConv(dataset.num_features, 16, normalize=False)\n",
        "        self.conv2 = GCNConv(16, dataset.num_classes, normalize=False)\n",
        "\n",
        "    def forward(self, x, edge_index, edge_weight):\n",
        "        x = F.relu(self.conv1(x, edge_index, edge_weight))\n",
        "        x = F.dropout(x, training=self.training)\n",
        "        x = self.conv2(x, edge_index, edge_weight)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = Net().to(device)\n",
        "data = data.to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "x, edge_index, edge_weight = data.x, data.edge_index, data.edge_weight\n",
        "\n",
        "num_epochs = 200\n",
        "\n",
        "for epoch in tqdm(range(num_epochs)):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    log_logits = model(x, edge_index, edge_weight)\n",
        "    loss = F.nll_loss(log_logits[data.train_mask], data.y[data.train_mask])\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "explainer = GNNExplainer(model, epochs=200, return_type='log_prob')\n",
        "node_idx = 10 #node to explain\n",
        "node_feat_mask, edge_mask = explainer.explain_node(node_idx, x, edge_index,\n",
        "                                                   edge_weight=edge_weight)\n",
        "ax, G = explainer.visualize_subgraph(node_idx, edge_index, edge_mask, y=data.y)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "QtI-ThydyeWZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}